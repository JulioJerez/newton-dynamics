<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Network Layers - dBrain Neural Network Library</title>
    <style>
        :root {
            --primary-color: #7c3aed;
            --secondary-color: #6d28d9;
            --bg-color: #f8fafc;
            --code-bg: #1e293b;
            --border-color: #e2e8f0;
            --text-color: #334155;
            --heading-color: #0f172a;
        }

        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.7;
            color: var(--text-color);
            background: var(--bg-color);
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }

        header {
            background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
            color: white;
            padding: 3rem 2rem;
            margin-bottom: 2rem;
        }

        header h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }

        header p {
            opacity: 0.9;
            font-size: 1.1rem;
        }

        nav.toc {
            background: white;
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 2rem;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }

        nav.toc h2 {
            margin-bottom: 1rem;
            color: var(--heading-color);
            border-bottom: 2px solid var(--primary-color);
            padding-bottom: 0.5rem;
            font-size: 1.25rem;
        }

        nav.toc ul {
            list-style: none;
            columns: 2;
        }

        nav.toc li {
            margin-bottom: 0.5rem;
        }

        nav.toc a {
            color: var(--primary-color);
            text-decoration: none;
        }

        nav.toc a:hover {
            text-decoration: underline;
        }

        section {
            background: white;
            border-radius: 8px;
            padding: 2rem;
            margin-bottom: 2rem;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }

        h2 {
            color: var(--heading-color);
            font-size: 1.75rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid var(--primary-color);
        }

        h3 {
            color: var(--heading-color);
            font-size: 1.35rem;
            margin-top: 1.5rem;
            margin-bottom: 0.75rem;
        }

        h4 {
            color: var(--heading-color);
            font-size: 1.1rem;
            margin-top: 1.25rem;
            margin-bottom: 0.5rem;
        }

        p {
            margin-bottom: 1rem;
        }

        pre {
            background: var(--code-bg);
            color: #e2e8f0;
            padding: 1.25rem;
            border-radius: 6px;
            overflow-x: auto;
            margin: 1rem 0;
            font-family: 'SF Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            background: #ede9fe;
            color: #6d28d9;
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: 'SF Mono', 'Fira Code', Consolas, monospace;
            font-size: 0.9em;
        }

        pre code {
            background: none;
            color: inherit;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
        }

        th, td {
            border: 1px solid var(--border-color);
            padding: 0.75rem;
            text-align: left;
        }

        th {
            background: #f5f3ff;
            font-weight: 600;
        }

        .note {
            background: #faf5ff;
            border-left: 4px solid var(--primary-color);
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 0 4px 4px 0;
        }

        ul, ol {
            margin: 1rem 0 1rem 1.5rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        .back-link {
            display: inline-block;
            margin-bottom: 1rem;
            color: var(--primary-color);
            text-decoration: none;
        }

        .back-link:hover {
            text-decoration: underline;
        }

        .class-header {
            background: #f5f3ff;
            padding: 1rem;
            border-radius: 6px;
            margin-bottom: 1rem;
            font-family: 'SF Mono', 'Fira Code', Consolas, monospace;
        }

        .class-header code {
            background: none;
            font-size: 1.1em;
        }

        .formula {
            background: #f8f9fa;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            font-family: 'SF Mono', 'Fira Code', Consolas, monospace;
            margin: 0.5rem 0;
            display: inline-block;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>Neural Network Layers</h1>
            <p>Linear, activation, convolutional, and dropout layers</p>
        </div>
    </header>

    <div class="container">
        <a href="index.html" class="back-link">&larr; Back to dBrain Documentation</a>

        <nav class="toc">
            <h2>Contents</h2>
            <ul>
                <li><a href="#base-layer">1. Base Layer Class</a></li>
                <li><a href="#linear">2. Linear Layer</a></li>
                <li><a href="#activation">3. Activation Layers</a></li>
                <li><a href="#convolutional">4. Convolutional Layers</a></li>
                <li><a href="#image">5. Image Processing Layers</a></li>
                <li><a href="#dropout">6. Dropout Layers</a></li>
            </ul>
        </nav>

        <section id="base-layer">
            <h2>1. Base Layer Class</h2>
            <div class="class-header">
                <code>class ndBrainLayer : public ndClassAlloc</code>
            </div>
            <p>
                All neural network layers inherit from <code>ndBrainLayer</code>, which defines
                the common interface for forward pass, backpropagation, and weight management.
            </p>

            <h3>Key Virtual Methods</h3>
            <table>
                <tr>
                    <th>Method</th>
                    <th>Description</th>
                </tr>
                <tr>
                    <td><code>GetInputSize()</code></td>
                    <td>Number of input neurons/elements</td>
                </tr>
                <tr>
                    <td><code>GetOutputSize()</code></td>
                    <td>Number of output neurons/elements</td>
                </tr>
                <tr>
                    <td><code>HasParameters()</code></td>
                    <td>Returns true if layer has trainable weights</td>
                </tr>
                <tr>
                    <td><code>InitWeights()</code></td>
                    <td>Initialize weights (Xavier/He initialization)</td>
                </tr>
                <tr>
                    <td><code>MakePrediction(in, out)</code></td>
                    <td>Forward pass computation</td>
                </tr>
                <tr>
                    <td><code>InputDerivative(...)</code></td>
                    <td>Compute gradient w.r.t. input</td>
                </tr>
                <tr>
                    <td><code>CalculateParamGradients(...)</code></td>
                    <td>Compute gradients for weight update</td>
                </tr>
            </table>

            <h3>Weight Initialization</h3>
<pre><code>// Default initialization (library chooses appropriate method)
layer->InitWeights();

// He initialization (good for ReLU networks)
layer->InitWeights_he();

// Xavier initialization (good for tanh/sigmoid networks)
layer->InitWeights_xavier();</code></pre>
        </section>

        <section id="linear">
            <h2>2. Linear Layer</h2>
            <div class="class-header">
                <code>class ndBrainLayerLinear : public ndBrainLayer</code>
            </div>
            <p>
                Fully-connected (dense) layer that performs an affine transformation:
            </p>
            <div class="formula">output = weights * input + bias</div>

            <h3>Constructor</h3>
<pre><code>// Create linear layer: inputSize -> outputSize
ndBrainLayerLinear* linear = new ndBrainLayerLinear(128, 64);

// Get dimensions
ndInt32 inputs = linear->GetInputSize();   // 128
ndInt32 outputs = linear->GetOutputSize(); // 64</code></pre>

            <h3>Accessing Weights</h3>
<pre><code>// Get weight matrix (outputSize x inputSize)
ndBrainMatrix* weights = linear->GetWeights();

// Get bias vector (outputSize elements)
ndBrainVector* bias = linear->GetBias();

// Modify weights manually (if needed)
(*weights)[0][0] = 0.5f;
(*bias)[0] = 0.1f;</code></pre>

            <h3>Forward Pass</h3>
<pre><code>ndBrainVector input(128);
ndBrainVector output(64);
// ... fill input ...

linear->MakePrediction(input, output);
// output = weights * input + bias</code></pre>

            <div class="note">
                <strong>GPU Support:</strong> Linear layers have full GPU support with tiled
                matrix multiplication for efficient computation on large networks.
            </div>
        </section>

        <section id="activation">
            <h2>3. Activation Layers</h2>
            <div class="class-header">
                <code>class ndBrainLayerActivation : public ndBrainLayer</code>
            </div>
            <p>
                Activation layers apply element-wise non-linear functions. They have no
                trainable parameters.
            </p>

            <h3>Available Activations</h3>
            <table>
                <tr>
                    <th>Class</th>
                    <th>Function</th>
                    <th>Use Case</th>
                </tr>
                <tr>
                    <td><code>ndBrainLayerActivationRelu</code></td>
                    <td>max(0, x)</td>
                    <td>Default choice for hidden layers</td>
                </tr>
                <tr>
                    <td><code>ndBrainLayerActivationLeakyRelu</code></td>
                    <td>x if x > 0, else alpha*x</td>
                    <td>Prevents "dying ReLU" problem</td>
                </tr>
                <tr>
                    <td><code>ndBrainLayerActivationElu</code></td>
                    <td>x if x > 0, else alpha*(exp(x)-1)</td>
                    <td>Smooth, handles negative values</td>
                </tr>
                <tr>
                    <td><code>ndBrainLayerActivationTanh</code></td>
                    <td>tanh(x)</td>
                    <td>Output range [-1, 1]</td>
                </tr>
                <tr>
                    <td><code>ndBrainLayerActivationSigmoid</code></td>
                    <td>1/(1+exp(-x))</td>
                    <td>Output range [0, 1], binary classification</td>
                </tr>
                <tr>
                    <td><code>ndBrainLayerActivationSoftmax</code></td>
                    <td>exp(x_i)/sum(exp(x))</td>
                    <td>Multi-class classification output</td>
                </tr>
                <tr>
                    <td><code>ndBrainLayerActivationLinear</code></td>
                    <td>x (identity)</td>
                    <td>Regression output, skip connections</td>
                </tr>
            </table>

            <h3>Usage</h3>
<pre><code>// ReLU activation for 64 neurons
ndBrainLayerActivationRelu* relu = new ndBrainLayerActivationRelu(64);

// Tanh activation
ndBrainLayerActivationTanh* tanh = new ndBrainLayerActivationTanh(64);

// Softmax for classification (10 classes)
ndBrainLayerActivationSoftmax* softmax = new ndBrainLayerActivationSoftmax(10);

// Add to network
brain.AddLayer(new ndBrainLayerLinear(128, 64));
brain.AddLayer(new ndBrainLayerActivationRelu(64));
brain.AddLayer(new ndBrainLayerLinear(64, 10));
brain.AddLayer(new ndBrainLayerActivationSoftmax(10));</code></pre>

            <h3>ReLU Variants</h3>
<pre><code>// Standard ReLU: max(0, x)
brain.AddLayer(new ndBrainLayerActivationRelu(neurons));

// Leaky ReLU: prevents dead neurons
// f(x) = x if x > 0, else alpha * x
brain.AddLayer(new ndBrainLayerActivationLeakyRelu(neurons));

// ELU: smooth alternative
// f(x) = x if x > 0, else alpha * (exp(x) - 1)
brain.AddLayer(new ndBrainLayerActivationElu(neurons));</code></pre>

            <div class="note">
                <strong>Recommendation:</strong> Use ReLU for hidden layers in most cases.
                For output layers, use Softmax for classification or Linear for regression.
            </div>
        </section>

        <section id="convolutional">
            <h2>4. Convolutional Layers</h2>
            <div class="class-header">
                <code>class ndBrainLayerConvolutional_2d : public ndBrainLayer</code>
            </div>
            <p>
                2D convolutional layer for processing image-like data. Applies learnable
                filters to extract spatial features.
            </p>

            <h3>Constructor</h3>
<pre><code>// Parameters:
// - inputWidth: image width
// - inputHeight: image height
// - inputChannels: number of input channels (e.g., 3 for RGB)
// - kernelSize: filter size (e.g., 3 for 3x3 filter)
// - outputChannels: number of output feature maps

ndBrainLayerConvolutional_2d* conv = new ndBrainLayerConvolutional_2d(
    28,  // input width
    28,  // input height
    1,   // input channels (grayscale)
    3,   // kernel size (3x3)
    32   // output channels
);</code></pre>

            <h3>Output Dimensions</h3>
<pre><code>ndInt32 outWidth = conv->GetOutputWidth();
ndInt32 outHeight = conv->GetOutputHeight();
ndInt32 outChannels = conv->GetOutputChannels();
ndInt32 filterSize = conv->GetFilterSize();</code></pre>

            <h3>Building a CNN</h3>
<pre><code>ndBrain cnn;

// Input: 28x28x1 (e.g., MNIST)
// Conv1: 28x28x1 -> 26x26x32
cnn.AddLayer(new ndBrainLayerConvolutional_2d(28, 28, 1, 3, 32));
cnn.AddLayer(new ndBrainLayerActivationRelu(26 * 26 * 32));

// Pooling: 26x26x32 -> 13x13x32
cnn.AddLayer(new ndBrainLayerImagePolling_2x2(26, 26, 32));

// Conv2: 13x13x32 -> 11x11x64
cnn.AddLayer(new ndBrainLayerConvolutional_2d(13, 13, 32, 3, 64));
cnn.AddLayer(new ndBrainLayerActivationRelu(11 * 11 * 64));

// Flatten and classify
cnn.AddLayer(new ndBrainLayerLinear(11 * 11 * 64, 128));
cnn.AddLayer(new ndBrainLayerActivationRelu(128));
cnn.AddLayer(new ndBrainLayerLinear(128, 10));
cnn.AddLayer(new ndBrainLayerActivationSoftmax(10));

cnn.InitWeights();</code></pre>

            <h3>Cross-Correlation Layer</h3>
            <div class="class-header">
                <code>class ndBrainLayerCrossCorrelation_2d : public ndBrainLayer</code>
            </div>
            <p>
                Similar to convolution but without flipping the kernel. In practice,
                many frameworks call this "convolution" since learned kernels make
                the distinction irrelevant.
            </p>
        </section>

        <section id="image">
            <h2>5. Image Processing Layers</h2>

            <h3>Padding Layer</h3>
            <div class="class-header">
                <code>class ndBrainLayerImagePadding : public ndBrainLayer</code>
            </div>
            <p>
                Adds zero-padding around images to maintain spatial dimensions
                after convolution.
            </p>
<pre><code>// Add 1-pixel padding around 26x26x32 feature maps
// Result: 28x28x32
ndBrainLayerImagePadding* pad = new ndBrainLayerImagePadding(26, 26, 32, 1);</code></pre>

            <h3>Pooling Layer</h3>
            <div class="class-header">
                <code>class ndBrainLayerImagePolling_2x2 : public ndBrainLayer</code>
            </div>
            <p>
                2x2 max pooling for spatial downsampling. Reduces width and height by half.
            </p>
<pre><code>// Pool 28x28x32 -> 14x14x32
ndBrainLayerImagePolling_2x2* pool = new ndBrainLayerImagePolling_2x2(28, 28, 32);

// In a network
brain.AddLayer(new ndBrainLayerConvolutional_2d(28, 28, 1, 3, 32));
brain.AddLayer(new ndBrainLayerActivationRelu(26 * 26 * 32));
brain.AddLayer(new ndBrainLayerImagePolling_2x2(26, 26, 32));  // -> 13x13x32</code></pre>
        </section>

        <section id="dropout">
            <h2>6. Dropout Layers</h2>
            <p>
                Dropout is a regularization technique that randomly sets neurons to zero
                during training, preventing overfitting.
            </p>

            <h3>Linear with Dropout</h3>
            <div class="class-header">
                <code>class ndBrainLayerLinearWithDropOut : public ndBrainLayerLinear</code>
            </div>
<pre><code>// Linear layer with integrated dropout
ndBrainLayerLinearWithDropOut* dropoutLinear =
    new ndBrainLayerLinearWithDropOut(128, 64);</code></pre>

            <h3>Convolutional with Dropout</h3>
            <div class="class-header">
                <code>class ndBrainLayerConvolutionalWithDropOut_2d : public ndBrainLayerConvolutional_2d</code>
            </div>
<pre><code>// Conv layer with dropout
ndBrainLayerConvolutionalWithDropOut_2d* dropoutConv =
    new ndBrainLayerConvolutionalWithDropOut_2d(28, 28, 1, 3, 32);</code></pre>

            <h3>Applying Dropout Rate</h3>
<pre><code>// Set dropout rate for the network (0.0 to 1.0)
// 0.2 means 20% of neurons are randomly dropped
brain.ApplyDropOutRate(0.2f);

// Reset dropout (disable for inference)
brain.ResetDropOut();</code></pre>

            <div class="note">
                <strong>Training vs Inference:</strong> Dropout should only be active during
                training. Call <code>ResetDropOut()</code> before making predictions on
                validation/test data.
            </div>
        </section>

        <section>
            <h2>Layer Summary</h2>
            <table>
                <tr>
                    <th>Layer Type</th>
                    <th>Has Weights</th>
                    <th>GPU Support</th>
                </tr>
                <tr>
                    <td>ndBrainLayerLinear</td>
                    <td>Yes</td>
                    <td>Yes</td>
                </tr>
                <tr>
                    <td>ndBrainLayerActivation*</td>
                    <td>No</td>
                    <td>Yes</td>
                </tr>
                <tr>
                    <td>ndBrainLayerConvolutional_2d</td>
                    <td>Yes</td>
                    <td>Partial</td>
                </tr>
                <tr>
                    <td>ndBrainLayerImagePolling_2x2</td>
                    <td>No</td>
                    <td>Partial</td>
                </tr>
                <tr>
                    <td>ndBrainLayerLinearWithDropOut</td>
                    <td>Yes</td>
                    <td>Yes</td>
                </tr>
            </table>
        </section>

        <section>
            <h2>Next Steps</h2>
            <ul>
                <li><a href="networks.html">Building Networks</a> - Compose layers into networks</li>
                <li><a href="training.html">Training Pipeline</a> - Train your networks</li>
            </ul>
        </section>
    </div>
</body>
</html>
